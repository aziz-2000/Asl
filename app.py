%%writefile app.py
import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
import av
import cv2
import numpy as np
from tensorflow.keras.models import load_model
from PIL import Image, ImageDraw, ImageFont
import arabic_reshaper
from bidi.algorithm import get_display

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
IMAGE_SIZE = 64

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
@st.cache_resource
def load_asl_model():
    model = load_model("asl_model.h5")
    return model

model = load_asl_model()

# Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø¹ÙƒØ³ÙŠ Ù„Ù„Ø­Ø±ÙˆÙ
label_map_ar = [
    'Ø¹', 'Ø§Ù„', 'Ø£', 'Ø¨', 'Ø¯', 'Ø¶', 'Ù', 'Øº', 'Ø­', 'Ù‡',
    'Ø¬', 'Ùƒ', 'Ø®', 'Ù„Ø§', 'Ù„', 'Ù…', 'Ù†', 'Ù‚', 'Ø±', 'Øµ',
    'Ø³', 'Ø´', 'Ø·', 'Øª', 'Ø©', 'Ø°', 'Ø«', 'Ùˆ', 'ÙŠ', 'Ø¸', 'Ø²'
]

reverse_label_map = {i: label for i, label in enumerate(sorted(label_map_ar))}

# Ø¯Ø§Ù„Ø© Ù„ÙƒØªØ§Ø¨Ø© Ù†Øµ Ø¹Ø±Ø¨ÙŠ Ø¹Ù„Ù‰ ØµÙˆØ±Ø© OpenCV Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Pillow
def draw_arabic_text(img, text, position, font_path="Amiri-Regular.ttf", font_size=36, color=(0, 255, 0)):
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    try:
        font = ImageFont.truetype(font_path, font_size)
    except:
        font = ImageFont.load_default()
    draw.text(position, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

# ÙˆØ§Ø¬Ù‡Ø© Streamlit
st.title("ğŸ”¤ Ù…ØªØ±Ø¬Ù… Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© - ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„ÙˆÙŠØ¨ Ø£Ùˆ ØµÙˆØ±Ø©")
st.markdown("Ø¥Ø¨Ø¯Ø§Ø¹ Ø¨Ù„Ø§ ØµÙˆØª*")

# ÙˆØ¸ÙŠÙØ© Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø­Ø±Ù Ù…Ù† ØµÙˆØ±Ø©
def predict_from_image(image: Image.Image):
    gray = image.convert('L')
    resized = gray.resize((IMAGE_SIZE, IMAGE_SIZE))
    img_array = np.array(resized).reshape(1, IMAGE_SIZE, IMAGE_SIZE, 1) / 255.0
    prediction = model.predict(img_array)
    pred_label = reverse_label_map[np.argmax(prediction)]
    return pred_label

# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙˆØ¶Ø¹: ÙƒØ§Ù…ÙŠØ±Ø§ Ø£Ùˆ ØµÙˆØ±Ø©
mode = st.radio("Ø§Ø®ØªØ± Ø§Ù„Ù…ØµØ¯Ø±:", ["ğŸ“· ÙƒØ§Ù…ÙŠØ±Ø§", "ğŸ–¼ï¸ ØµÙˆØ±Ø©"])

if mode == "ğŸ“· ÙƒØ§Ù…ÙŠØ±Ø§":
    class SignLanguageTransformer(VideoTransformerBase):
        def transform(self, frame: av.VideoFrame) -> np.ndarray:
            img = frame.to_ndarray(format="bgr24")
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            h, w = gray.shape
            center_crop = gray[h//2-100:h//2+100, w//2-100:w//2+100]

            try:
                resized = cv2.resize(center_crop, (IMAGE_SIZE, IMAGE_SIZE)).reshape(1, IMAGE_SIZE, IMAGE_SIZE, 1) / 255.0
                prediction = model.predict(resized)
                pred_label = reverse_label_map[np.argmax(prediction)]

                reshaped_text = arabic_reshaper.reshape(f"Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: {pred_label}")
                bidi_text = get_display(reshaped_text)
                img = draw_arabic_text(img, bidi_text, (10, 40), font_size=36)

                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø´Ø¹Ø§Ø±
                logo_text = arabic_reshaper.reshape("Ø¥Ø¨Ø¯Ø§Ø¹ Ø¨Ù„Ø§ ØµÙˆØª")
                logo_bidi = get_display(logo_text)
                img = draw_arabic_text(img, logo_bidi, (img.shape[1]-300, 10), font_size=28, color=(255, 255, 255))

                # Ù…Ø±Ø¨Ø¹ Ø§Ù„ÙŠØ¯
                cv2.rectangle(img, (w//2-100, h//2-100), (w//2+100, h//2+100), (255, 0, 0), 2)
            except Exception as e:
                print("Ø®Ø·Ø£:", e)

            return img

    webrtc_streamer(key="sign-detect", video_transformer_factory=SignLanguageTransformer)

else:
    uploaded_file = st.file_uploader("Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© Ù„Ù„ÙŠØ¯ Ø¨Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø©", type=["jpg", "jpeg", "png"])
    if uploaded_file:
        img = Image.open(uploaded_file)
        st.image(img, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©", use_column_width=True)

        pred_label = predict_from_image(img)
        reshaped_text = arabic_reshaper.reshape(f"Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: {pred_label}")
        bidi_text = get_display(reshaped_text)
        st.markdown(f"### âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø©: **{bidi_text}**")
